<!doctype html>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" type="text/css" href="../../styles/commonstyles.css" />
<link rel="stylesheet" href="../../styles/blogstyle.css" />
<link rel="stylesheet" href="../../styles/iconbarstyle.css" />
<link rel="stylesheet" href="../../styles/toc.css" />
<link rel="stylesheet" href="../../styles/figcaptionstyles.css" />

<html lang="en-us">
  <head>
    <title>Databases</title>
  </head>

  <body>
    <div class="icon-bar">
      <a class="active" href="../../index.html"><i class="fa fa-home"></i></a>
    </div>

    <div style="padding: 20px">
      <p>05 May 2025</p>
      <p>
        <a href="../11_ml2/11.html" style="color: firebrick"> &#8617; Previous</a>
      </p>

      <hr />

      <div id="toc"></div>
      <script src="../../scripts/toc.js"></script>
    </div>
    <h2>Neural networks</h2>

    <p>
      When neural networks were first invented many decades ago, the original motivation was to write software that
      could mimic how the human brain or how the biological brain learns and thinks. Work in neural networks had started
      back in the 1950s.
    </p>
    <figure>
      <img src="biological_neuron.png" alt="" width="750" height="300" />
      <figcaption>Biological neuron (credit Andrew NG).</figcaption>
    </figure>
    <p>
      When you're building an artificial neural network or deep learning algorithm, rather than building one neuron at a
      time, you often simulate many such neurons at the same time
    </p>
    <figure>
      <img src="many_neurons.png" alt="" width="300" height="250" />
      <figcaption>Many neurons (credit Andrew NG).</figcaption>
    </figure>
    <p>
      Those of us that do research in deep learning have shifted away from looking to biological motivation that much.
      But instead, they're just using engineering principles to figure out how to build algorithms that are more
      effective.
    </p>

    <p>Let us use an example from demand prediction</p>

    <figure>
      <img src="demand_prediction.png" alt="" width="650" height="300" />
      <figcaption>Demand prediction (credit Andrew NG).</figcaption>
    </figure>

    <p>
      Let us use the alphabet “a” to denote the output of this logistic regression algorithm. The term a stands for
      activation, and it's a term from neuroscience. It refers to how much a neuron is sending a high output to other
      neurons downstream from it.
    </p>

    <p>
      This unit of logistic regression algorithm, can be thought of as a very simplified model of a single neuron in the
      brain. Building a neural network now it just requires taking a bunch of these neurons and wiring them together or
      putting them together. A layer is a grouping of neurons which takes as input the same or similar features, and
      that in turn outputs a few numbers together.
    </p>

    <figure>
      <img src="neuralnet_layer.png" alt="" width="650" height="300" />
      <figcaption>Layers (credit Andrew NG).</figcaption>
    </figure>

    <p>Hidden Layer</p>

    <p>
      Your data set tells you what is x and what is y. ie, you have information on the correct inputs and the correct
      outputs. But your dataset doesn't tell you what are the correct values for affordability, awareness, and perceived
      quality. The correct values for those are hidden. You don't see them in the training set, which is why this layer
      in the middle is called a hidden layer.
    </p>
    <p>
      The input layer has a vector of features (four numbers in this example) it is input to the hidden layer, which
      outputs three numbers.
    </p>
    <figure>
      <img src="neuralnet_examples.png" alt="" width="450" height="500" />
      <figcaption>Layers (credit Andrew NG).</figcaption>
    </figure>
    <p>
      When you're building your own neural network, one of the decisions you need to make is how many hidden layers do
      you want and how many neurons do you want each hidden layer to have. It is a question of the architecture of the
      neural network. But choosing the right number of hidden layers and number of hidden units per layer can have an
      impact on the performance of a learning algorithm as well.
    </p>

    <p>The term multilayer perceptron just refers to a neural network that looks like the above.</p>

    <h3>Recognizing Images</h3>
    <p>
      For building a face recognition application, you will train a neural network that takes as input a picture and
      outputs the identity of the person in the picture. This image example is 1,000 by 1,000 pixels. It is a 1000 by
      1000 matrix of pixel intensity values.
    </p>
    <figure>
      <img src="1000_x_1000.png" alt="" width="450" height="300" />
      <figcaption>Input Image (credit Andrew NG).</figcaption>
    </figure>

    <p>
      If you were to take these pixel intensity values and unroll them into a vector, you end up with a list or a vector
      of a million (1000x1000) pixel intensity values. The face recognition problem is thus about training a neural
      network that takes as input a feature vector with a million pixel values and outputs the identity of the person in
      the picture
    </p>
    <p>
      In the first hidden layer, you might find one neuron that is looking for the low vertical line or a vertical edge
      . A second neuron looking for a oriented line or oriented edge . The third neuron looking for a line at that
      orientation, and so on. The neural network is aggregating different parts of faces to then try to detect presence
      or absence of larger, coarser face shapes
    </p>
    <figure>
      <img src="face_recognition_working.png" alt="" width="800" height="400" />
      <figcaption>Input Image (credit Andrew NG).</figcaption>
    </figure>

    <p>
      The same learning algorithm is asked to detect cars, will then learn edges in the first layer. Pretty similar but
      then they'll learn to detect parts of cars in the second hidden layer and then more complete car shapes in the
      third hidden layer. Just by feeding it different data, the neural network automatically learns to detect very
      different features so as to try to make the predictions of car detection or person recognition.
    </p>
    <figure>
      <img src="car_classification.png" alt="" width="800" height="350" />
      <figcaption>Input Image (credit Andrew NG).</figcaption>
    </figure>
    <p>
      The fundamental building block of most modern neural networks is a layer of neurons. Take an example of four input
      features that were set to layer of three neurons in the hidden layer that then sends its output to output layer
      with just one neuron
    </p>

    <figure>
      <img src="3_1_layer.png" alt="" width="300" height="200" />
      <figcaption>Demand prediction - Layers (credit Andrew NG).</figcaption>
    </figure>

    <p>
      This hidden layer inputs four numbers and these four numbers are inputs to each of three neurons. Each of these
      three neurons is just implementing a little logistic regression function. Zooming in:
    </p>
    <figure>
      <img src="3_1_layer_zoom.png" alt="" width="600" height="300" />
      <figcaption>Demand prediction - Layers (credit Andrew NG).</figcaption>
    </figure>

    <p>
      Take the first neuron. It has two parameters, w and b. In this example, these three neurons output 0.3, 0.7, and
      0.2, and this vector of three numbers becomes the vector of activation values "a".
    </p>

    <figure>
      <img src="3_1_layer_zoom_activation.png" alt="" width="700" height="300" />
      <figcaption>Demand prediction - Layers (credit Andrew NG).</figcaption>
    </figure>
    <p>
      The input layer is also sometimes called layer 0. There are neural networks that can have dozens or even hundreds
      of layers.
    </p>

    <p>
      There's a final optional step that you can choose to implement or not, - if you want a binary prediction "is this
      a top seller? Yes or no?""
    </p>

    <figure>
      <img src="nn_binary_prediction.png" alt="" width="700" height="250" />
      <figcaption>Demand prediction - Layers (credit Andrew NG).</figcaption>
    </figure>

    <p>
      Every layer inputs a vector of numbers and applies a bunch of logistic regression units to it. Then computes
      another vector of numbers that then gets passed from layer to layer, until you get to the final output layers
      computation, which is the prediction of the neural network. Then you can either threshold at 0.5 or not to come up
      with the final prediction.
    </p>
    <figure>
      <img src="nn_4_layer.png" alt="" width="800" height="300" />
      <figcaption>4 layer neural network (credit Andrew NG).</figcaption>
    </figure>
    <p>Layer 0 is the input layer. Layer 1, 2, 3 are hidden layers. Layer 4 is the output layer:</p>

    <figure>
      <img src="nn_4_layer_2.png" alt="" width="450" height="300" />
      <figcaption>4 layer neural network (credit Andrew NG).</figcaption>
    </figure>

    <h2>Inference: making predictions (forward propagation)</h2>
    <p>Handwritten digit recognition:</p>

    <p>
      We have an image of a "one" in a grid/matrix of 8x8 = (64) pixel intensity values where 255 denotes a bright white
      pixel and zero would denote a black pixel. Different numbers are different shades of gray in between the shades of
      black and white. Given these 64 input features, we're going to use the neural network with two hidden layers.
      First hidden layer has 25 neurons or 25 units. Second hidden layer has 15 neurons or 15 units. And then finally
      the output layer.
    </p>

    <figure>
      <img src="handwritten_digit_1.png" alt="" width="700" height="300" />
      <figcaption>Handwritten digit recognition (credit Andrew NG).</figcaption>
    </figure>
    <p>where:</p>

    <figure>
      <img src="handwritten_digit_2.png" alt="" width="360" height="300" />
      <figcaption>Handwritten digit recognition (credit Andrew NG).</figcaption>
    </figure>

    <figure>
      <img src="handwritten_digit_3.png" alt="" width="700" height="350" />
      <figcaption>Handwritten digit recognition (credit Andrew NG).</figcaption>
    </figure>
    <p>
      This type of neural network architecture where you have more hidden units initially and then the number of hidden
      units decreases as you get closer to the output layer. There's also a pretty typical choice when choosing neural
      network architectures
    </p>
    <p>
      One of the remarkable things about neural networks is the same algorithm can be applied to so many different
      applications
    </p>
    <h3>Building a layer in tensor flow</h3>

    <p>
      TensorFlow is one of the leading frameworks to implementing deep learning algorithms. The other popular tool is
      PyTorch.
    </p>

    <p>
      Note that unfortunately there are some inconsistencies between how data is represented in NumPy and in TensorFlow.
      TensorFlow was designed to handle very large datasets and by representing the data in matrices instead of 1D
      arrays.
    </p>
    <p>
      A tensor is a data type that the TensorFlow team had created in order to store and carry out computations on
      matrices efficiently. So whenever you see tensor just think of that matrix in the picture. Technically a tensor is
      a little bit more general than the matrix.
    </p>
    <figure>
      <img src="tensorflow_1.png" alt="" width="500" height="400" />
      <figcaption>Tensorflow - Activation vector (credit Andrew NG).</figcaption>
    </figure>

    <figure>
      <img src="tensorflow_2.png" alt="" width="500" height="400" />
      <figcaption>Tensorflow - Activation vector(credit Andrew NG).</figcaption>
    </figure>

    <p>
      There's the TensorFlow way of representing the matrix and the NumPy way of representing matrix. This is an
      artifact of the history of how NumPy and TensorFlow were created
    </p>

    <p>
      If you want to do forward propogation, you initialize the data X create layer 1 then compute a1 , then create
      layer 2 and compute a2. So this was an explicit way of carrying out forward propogation one layer of computation
      at the time. Tensor flow has a different way of implementing forward propogation as well as learning
    </p>
    <hr />

    <p>
      <a href="../11_ml2/11.html" style="color: firebrick"> &#8617; Previous</a>
    </p>
    <p></p>
  </body>
</html>