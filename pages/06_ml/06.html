<!doctype html>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" type="text/css" href="../../styles/commonstyles.css" />
<link rel="stylesheet" href="../../styles/blogstyle.css" />
<link rel="stylesheet" href="../../styles/iconbarstyle.css" />
<link rel="stylesheet" href="../../styles/toc.css" />

<html lang="en-us">
  <head>
    <title>Machine Learning</title>
  </head>

  <body>
    <div class="icon-bar">
      <a class="active" href="../../index.html"><i class="fa fa-home"></i></a>
    </div>

    <div style="padding: 20px">
      <h1>Machine Learning</h1>

      <p>18 Apr 2025</p>
      <hr />

      <!-- Basic concepts of encryption in cryptography -->
      <h2>Table of contents</h2>

      <div id="toc"></div>

      <script src="../../scripts/toc.js"></script>

      <h2>Installing Packages</h2>
      <h3>Installing Anaconda</h3>

      <p>
        As first step, download the Anaconda package manager from https://www.anaconda.com. Then follow the installer UI
        instructions. After a successful installation, open Anaconda-Navigator App.
      </p>
      <h3>Installing Jupyter Notebook</h3>

      <p>
        From the Anaconda-Navigator, click on Jupyter Notebook Lauch button. Jupyter Notebook will be installed and a
        webpage will be opend. From now on , you can lauch Jupyter notebook from commandline by typing jypyter-notebook.
      </p>

      <h2>Machine Learning</h2>

      <p>
        Machine Learning Specialization from Andrew NG is a good reference to understand the basics of Machine learning.
      </p>

      <p>
        "Machine Learning is the field of study that gives computers the ability to learn withoit being explicitly
        programmed" - Arutur Samuel 1959
      </p>

      <h2>Machine Learning algorithms</h2>
      <ul>
        <li>
          Supervised Machine Learning
          <ul>
            <li>Regression</li>
            <li>Classification</li>
          </ul>
        </li>
        <li>
          Unupervised Machine Learning
          <ul>
            <li>Clustering</li>
            <li>Dimensionality Reduction</li>
          </ul>
        </li>
        <li>Recommender Systems</li>
        <li>Reinforcement learning</li>
      </ul>

      <h2>Supervised Machine Learning- Regression</h2>
      <ul>
        <li>
          The key characteristic of a supervised learning is that you give your learning algorithm examples to learn
          from.
        </li>
        <li>These alorithms help to learn input X to output Y mapping</li>
        <li>
          Examples
          <ul>
            <li>Smapl filters: X= Email ; Y = Spam (0/1)</li>
            <li>Speech recognition: X = Audio ; Y = Transcript</li>
            <li>Machine Translation: X = Englisg; Y = Spanish</li>
            <li>Online advertising : X = Ad, User-information ; Y = Click (0/1)</li>
            <li>Self driving car: X = Image, RadarInput; Y = Position of other cars</li>
            <li>Visual Inspection: X= Image of Phone ; Y = Detect (0/1)</li>
          </ul>
        </li>
        <li>
          In all these applications, you will train your model with examples of input X and the expected answers Y
          (labels)
        </li>
        <li>Example of Housing price based on known data</li>
      </ul>

      <figure>
        <img src="regression_housing_price_prediction.png" alt="Housing Price Data" width="500" height="300" />
        <figcaption>Housing Price Data(credit Andrew NG).</figcaption>
      </figure>

      <ul>
        <li>
          Now we have to predict housing price for a new input , say 1200 which is not in the data that we have. You
          need to find a mathematic function that best fit the data given.
          <ul>
            <li>
              For doing this, you need to decide a straight line, a curve or another function that is even more complex
              for the data.
            </li>
            <li>You also need an algorithm to choose the correct function to fit this data.</li>
          </ul>
        </li>
        <li>
          This housing price prediction is the particular type of supervised learning called regression. In regression
          we are trying to predict a number from infinitely many possible numbers, which could be 150,000 or 70,000 or
          183,000 or any other number in between.
        </li>
      </ul>
      <figure>
        <img src="regression_housing_math_function.png" alt="Housing Price Data" width="500" height="300" />
        <figcaption>Housing Price Prediction functions (credit Andrew NG).</figcaption>
      </figure>

      <h2>Supervised Machine Learning- Classification</h2>
      <ul>
        <li>
          Here predict only a small number of output categories (0 or 1 in this case) unlike regression where there are
          infinitely many.
        </li>
        <li>
          Classification predicts categories. It need not to be numbers - it can be non numeric also (predict cat or
          dog).
        </li>
        <li>Example : Identify if given tumor size is benign or not.</li>
        <li>X= input parameters ; Y = Classes or Categories.</li>
        <li>
          Classification can be more than 2 classes. For example, input parameter is the tumor size, and output classes
          are benign, malignant type1, and type2.
        </li>
      </ul>
      <figure>
        <img src="classification_multi_class.png" alt="Housing Price Data" width="400" height="100" />
        <figcaption>Single input, Multi class output (credit Andrew NG).</figcaption>
      </figure>
      <ul>
        <li>You can have also have multiple inputput paramters (X) like age, tumor size.</li>
      </ul>
      <figure>
        <img src="classification_multi_input_multi_class.png" alt="Housing Price Data" width="400" height="279" />
        <figcaption>Multi input, multi class output (credit Andrew NG).</figcaption>
      </figure>

      <ul>
        <li>
          To do this, algorithm finds some boundary that separates out malignant from benign ones. So the learning
          algorithm has to decide how to fit a boundary line through this data
        </li>
      </ul>
      <figure>
        <img src="classification_multi_input_multi_class_boundary.png" alt="Multi input" width="400" height="279" />
        <figcaption>Class boundary (credit Andrew NG).</figcaption>
      </figure>

      <h2>Unsupervised learning - Clustering</h2>

      <ul>
        <li>After supervised learning, the most widely used form of machine learning is unsupervised learning</li>
        <li>
          In unsupervised learning we are given data that isn't associated with any output labels Y. We dont mark the
          "right answers" for training the model. We're not asked to diagnose whether the tumor is benign or malignant,
          because we're not given any labels . Instead, we asked the algorithm to figure out all by yourself what's
          interesting. The algorithm will output the clusters it sees. This is a particular type of unsupervised
          learning, called a clustering algorithm that helps to find structure in the data given.
        </li>
      </ul>
      <figure>
        <img src="unsupervised_clustering.png" alt="Clustering" width="400" height="250" />
        <figcaption>Clustering (credit Andrew NG).</figcaption>
      </figure>

      <ul>
        <li>
          For example google news groups related news together. This is achived through clustering algorithms. The
          clustering algorithm figures out what words to use to form a cluster. It can also be used for grouping
          customers.
        </li>
      </ul>

      <h2>Unsupervised learning - Dimensionality Reduction</h2>

      <ul>
        <li>
          Dimensionality reduction lets you take a big data-set and almost magically compress it to a much smaller
          data-set while losing as little information as possible
        </li>
      </ul>

      <h2>More on Linear Regression</h2>
      <ul>
        <li>
          Linear regression is defined as an algorithm that provides a linear relationship between an independent
          variable (Y) and a dependent variable (X) to predict the outcome of future events. A sloped straight line
          represents the linear regression model
        </li>
      </ul>

      <figure>
        <img src="linear_regression_best_fit.png" alt="Clustering" width="500" height="400" />
        <figcaption>Linear regression best fit (credit www.spiceworks.com).</figcaption>
      </figure>
      <ul>
        <li>The linear regression model is computationally simple.</li>
      </ul>
      <h3>ML Terminology</h3>

      <figure>
        <img src="ml_terminology.png" alt="Clustering" width="700" height="400" />
        <figcaption>ML terminology (credit Andrew NG).</figcaption>
      </figure>

      <h3>Linear regression with 1 variable</h3>
      <ul>
        <li>
          To train the model, you feed the training set, both the input features and the output targets to your learning
          algorithm. Then your supervised learning algorithm will produce some function. Historically, this function
          used to be called a hypothesis, let us call it a function f . The job with f is to take a new input X and
          output , and a prediction, which is called Y-hat. In machine learning, the convention is that y-hat is the
          estimate or the prediction for y.
        </li>
        <li>
          The function f is called the model. X is called the input or the input feature and the output of the model is
          the prediction, Y-hat
        </li>

        <li>
          When the symbol is just the letter Y, then that refers to the target, which is the actual true value in the
          training set. In contrast, Y-hat is an estimate. It may or may not be the actual true value.
        </li>
        <li>
          Your model f, given the size, outputs the price which is the estimator, that is the prediction of what the
          true price will be.
        </li>
      </ul>

      <p>How are we going to represent the function f?</p>
      <figure>
        <img src="f_representation.png" alt="Clustering" width="320" height="200" />
        <figcaption>Representaion of f - Linear Regression with One Variable (credit Andrew NG).</figcaption>
      </figure>

      <ul>
        <li>
          This function is making predictions for the value of Y using a straight-line function of X. Sometimes you want
          to fit more complex non-linear functions as well, like a curve. A linear function is relatively simple and
          easy to work with, and will help you to get to more complex models that are non-linear.
        </li>
        <li>
          Above function has a single input variable or feature X, namely the size of the house. This is called a
          univariate linear regression.
        </li>
      </ul>

      <h2>Cost Function</h2>
      <p>
        The idea of a cost function is one of the most universal and important ideas in machine learning, and is used in
        both linear regression and in training many of the most advanced AI models in the world. Take an example of
        below dataset:
      </p>

      <figure>
        <img src="training_set_feet_vs_price.png" alt="Clustering" width="320" height="200" />
        <figcaption>Training set (credit Andrew NG).</figcaption>
      </figure>

      <p>
        Checking visually with scatterplots or statistically using correlation coefficients we can see that there is a
        linear relationship between the independent variables (x) and the dependent variable (y). So linear regression
        could be a choice. In order to implement linear regression the first key step is first to define a cost
        function. Cost function will tell us how well the model is doing so that we can try to get it to do better
      </p>

      <figure>
        <img src="training_set_feet_vs_price_model.png" alt="Clustering" width="320" height="200" />
        <figcaption>Model (credit Andrew NG).</figcaption>
      </figure>

      <p>
        The model we are going to choose is a function above, where w and b are parameters (coefficients / weights).
        Depending on the values you've chosen for w and b you get a different function f of x, which generates a
        different line on the graph
      </p>
      <ul>
        <li>f(x) = 0x + 1.5</li>
        <li>f(x) = 0.5x + 0</li>
        <li>f(x) = 0.5x + 1</li>
      </ul>
      <p>
        With linear regression, you want choose values for the parameters w and b so that the straight line you get from
        the function f somehow fits the data well
      </p>

      <figure>
        <img src="training_set_feet_vs_price_fitted.png" alt="Fitted Model" width="320" height="200" />
        <figcaption>Fitted Model (credit Andrew NG).</figcaption>
      </figure>
      <h3>Finding values for w and b</h3>
      <p>How to automatically find the value of w and b? From the training data you have:</p>

      <ul>
        <li>y<sup>(1)</sup> , x<sup>(1)</sup></li>
        <li>y<sup>(2)</sup> , x<sup>(2)</sup></li>
        <li>y<sup>(3)</sup> , x<sup>(3)</sup></li>
      </ul>
      <p>Predicting the the values (ŷ) using the model formula y= w*x + b :</p>

      <ul>
        <li>ŷ<sup>(1)</sup> = x<sup>(1)</sup> + b</li>
        <li>ŷ<sup>(2)</sup> = x<sup>(2)</sup> + b</li>
        <li>ŷ<sup>(3)</sup> = x<sup>(3)</sup> + b</li>
      </ul>
      <p>
        In above formula you used one of the w and b values. There are other w and b values you have to try. You have to
        find best value for w and b so that the the prediction ŷ is close to the training y value, for all training
        samples. To do that, you need to construct a cost function.
      </p>
      <h3>Cost function defintion</h3>
      <p>
        The cost function takes the prediction ŷ and compares it to the target y by taking (ŷ - y). This difference is
        called the error. Here we are measuring how far off to prediction is from the target. We use squared eror to
        avoid negative and positves mixup.
      </p>

      <figure>
        <img src="cost_function.png" alt="Cost function" width="320" height="150" />
        <figcaption>Cost function (credit Andrew NG).</figcaption>
      </figure>
      <p>
        Notice that if we have more training examples (m) is larger, and your cost function will calculate a bigger
        number since it is summing over more example. To build a cost function that doesn't automatically get bigger as
        the training set size gets larger we do below:
      </p>
      <figure>
        <img src="cost_function_1_by_m.png" alt="Cost function" width="320" height="150" />
        <figcaption>Cost function (credit Andrew NG).</figcaption>
      </figure>
      <p>
        By convention, the cost function that machine learning people use actually divides by 2 times m. The extra
        division by 2 is just meant to make some of our later calculations look neater, but the cost function still
        works whether you include this division by 2 or not
      </p>

      <figure>
        <img src="cost_function_final.png" alt="Squared error cost function" width="380" height="220" />
        <figcaption>Squared error cost function (credit Andrew NG).</figcaption>
      </figure>
      <p>
        In machine learning different people will use different cost functions for different applications, but the
        squared error cost function is by far the most commonly used one for linear regression
      </p>
      <p>
        We have to find values of w and b that make the cost function small (ie, find best parameters for your model).
        Linear regression would try to find values for w, and b, that make a J(w,b) be as small as possible. For
        example: J (0.5,0) = 0.58
      </p>

      <figure>
        <img src="J_computation.png" alt="J Computation" width="500" height="300" />
        <figcaption>J Computation (credit Andrew NG).</figcaption>
      </figure>
      <h3>Minimising cost function - Example using single parameter</h3>
      <p>
        Let us consider a cost function J with single parameter w. It is unlikely that the initial J(w) yoou tried gives
        the minimu possible value of J. Increment the values of (w and b) in a sequence, and plot corresponding J
        values. By computing a range of values, you can slowly trace out the cost function J :
      </p>

      <figure>
        <img src="cost_function_plot.png" alt="Cost function plot" width="400" height="300" />
        <figcaption>Cost function plot (credit Andrew NG).</figcaption>
      </figure>
      <p>
        Choosing a value of w and b that causes J (w) to be as small as possible is a good model. In other words, find
        the values of w and b that minimize J.
      </p>

      <h3>Cost function with more parameters</h3>
      <p>
        It is easy to plot when there is 1 parameter (w) . It is complex to plot J since there are 2 parameters (w and
        b). It turns out that the cost function shape like a soup bowl, except in three dimensions instead of two.
      </p>
      <figure>
        <img src="cost_function_formula.png" alt="Cost function formula" width="500" height="250" />
        <figcaption>Cost function formula (credit Andrew NG).</figcaption>
      </figure>
      <p>The plot of cost fuinction (for a range of J values) is below:</p>
      <figure>
        <img src="cost_function_2_parameters.png" alt="Cost function with 2 parameters" width="560" height="400" />
        <figcaption>Cost function with 2 parameters (credit Andrew NG).</figcaption>
      </figure>
      <h3>Alternate way of plotting cost function</h3>
      <p>
        There's another way of plotting the cost function J which is, rather than using these 3D-surface plots. We can
        plot it using something called a contour plot
      </p>
      <figure>
        <img src="contour_plot.png" alt="Cost function with 2 parameters" width="560" height="400" />
        <figcaption>Contour Plot (credit Andrew NG).</figcaption>
      </figure>
      <p>
        All of the points in a ring will have the same value for the cost function J, even though they have different
        values for w and b. b=0, w = 360 example
      </p>

      <figure>
        <img src="coutour_plot_example.png" alt="Contour plot example" width="700" height="550" />
        <figcaption>Contour plot example (credit Andrew NG).</figcaption>
      </figure>
      <p>
        What you really want is an efficient algorithm that automatically finds the values of parameters w and b that
        give you the best fit line that minimizes the cost function J. There is an algorithm for doing this (training
        model) called gradient descent. This is one of the most important algorithms in machine learning. It is not just
        linear regression, but also used in some of the biggest and most complex models in all of AI.
      </p>

      <pre>
x_train = np.array([1.0, 2.0])        #(size in 1000 square feet)
y_train = np.array([300.0, 500.0])    #(price in 1000s of dollars)


def compute_cost(x, y, w, b): 
    """
        Computes the cost function for linear regression.

        Args:
          x (ndarray (m,)): Data, m examples 
          y (ndarray (m,)): target values
          w,b (scalar)    : model parameters  

        Returns
            total_cost (float): The cost of using w,b 
            as the parameters for linear regression
            to fit the data points in x and y
    """
    
    # number of training examples
    m = x.shape[0] 
    
    cost_sum = 0 
    for i in range(m): 
        f_wb = w * x[i] + b   
        cost = (f_wb - y[i]) ** 2  
        cost_sum = cost_sum + cost  
    total_cost = (1 / (2 * m)) * cost_sum  

    return total_cost
</pre
      >
      <p>
        The fact that the cost function squares the loss ensures that the 'error surface' is convex like a soup bowl. It
        will always have a minimum that can be reached by following the gradient in all dimensions.
      </p>

      <h2>Gradient descent</h2>
      <p>
        Gradient descent is a systematic way to find the values of w and b, that results in the smallest possible cost.
        Gradient descent is used all over the place in machine learning, not just for linear regression, but for
        training of the most advanced neural network models, also called deep learning models. Gradient descent is an
        algorithm that you can use to try to minimize any function, not just a cost function for linear regression -
        gradient descent more general.
      </p>

      <p>
        Start off with some initial guesses of w and b. In linear regression, it won't matter too much what the initial
        value are, so a common choice is to set them both to 0. For example, you can set w to 0 and b to 0 as the
        initial guess.
      </p>
      <figure>
        <img src="gradient_descent_algotithm.png" alt="Gradient descent" width="400" height="200" />
        <figcaption>Gradient descent (credit Andrew NG).</figcaption>
      </figure>
      <p>
        For linear regression with the squared error cost function, you always end up with a bow shape or a hammock
        shape. Some J functions may not be a bow shape or a hammock shape, it is possible for there to be more than one
        possible minimum (not a squared error cost function). This is a type of cost function you might get if you're
        training a neural network model. Your goal is to start up here and get to the bottom of one of these valleys as
        efficiently as possible by taking small steps.
      </p>
      <figure>
        <img src="gradient_discent_run_down.png" alt="Gradient descent" width="400" height="400" />
        <figcaption>Gradient descent run down (credit Andrew NG).</figcaption>
      </figure>
      <p>
        gradient descent has an interesting property- If you were to run gradient descent this second time, starting
        just a couple steps in the right of where we did it the first time, then you end up in a totally different
        valley
      </p>
      <figure>
        <img src="gradient_discent_different_valley.png" alt="Gradient descent" width="600" height="400" />
        <figcaption>Gradient descent run down (credit Andrew NG).</figcaption>
      </figure>
      <p>
        Because if you start going down the first valley, gradient descent won't lead you to the second valley, and the
        same is true if you started going down the second valley- you stay in that second minimum and not find your way
        into the first local minimum
      </p>
    </div>
  </body>
</html>