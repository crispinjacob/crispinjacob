<!doctype html>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" type="text/css" href="../../styles/commonstyles.css" />
<link rel="stylesheet" href="../../styles/blogstyle.css" />
<link rel="stylesheet" href="../../styles/iconbarstyle.css" />
<link rel="stylesheet" href="../../styles/toc.css" />
<link rel="stylesheet" href="../../styles/figcaptionstyles.css" />

<html lang="en-us">
  <head>
    <title>Databases</title>
  </head>

  <body>
    <div class="icon-bar">
      <a class="active" href="../../index.html"><i class="fa fa-home"></i></a>
    </div>

    <div style="padding: 20px">
      <p>02 May 2025</p>
      <p>
        <a href="../06_ml/06.html" style="color: firebrick"> &#8617; Previous</a>
      </p>

      <hr />

      <div id="toc"></div>
      <script src="../../scripts/toc.js"></script>
    </div>
    <h2>Feature scaling</h2>
    <p>Consider below model for housing price</p>
    <ul>
      <li>price = w<sub>(1)</sub> x<sub>(1)</sub> + w<sub>(2)</sub> x<sub>(2)</sub> + b</li>
      <li>x<sub>(1)</sub> - size in sq feet (range: 300-2000). Range is large.</li>
      <li>x<sub>(2)</sub> - bedrooms (range:0-5). Range is small.</li>
    </ul>
    <p>
      Applying the largest feature range (2000 and 5) into the model, with model parameters w<sub>(1)</sub> = 50 ,
      w<sub>(2)</sub>= 0.1 , b = 50 :
    </p>

    <ul>
      <li>price = w<sub>(1)</sub> x <sub>(1)</sub> + w<sub>(2)</sub> x <sub>(2)</sub> + b</li>
      <li>
        price = 50 * 2000 + 0.1 * 5 + 50 = $100,050.5. This is far from actual price of 500k dollars. So this is not a
        good parameter choice
      </li>
    </ul>

    <p>Another example with w<sub>(1)</sub> = 0.1 , w<sub>(2)</sub>= 50 , b = 50 :</p>

    <ul>
      <li>price = w<sub>(1)</sub> x <sub>(1)</sub> + w<sub>(2)</sub> x <sub>(2)</sub> + b</li>
      <li>price = 0.1 * 2000 + 0.1 * 5 + 50 = $500,000. This is more reasonable</li>
    </ul>

    <p>
      When the possible range of values of a feature is large a good model will learn to choose a relatively small
      parameter value, like 0.1. Likewise, when the possible values of the feature are small then a reasonable parameter
      value will be relatively large like 50.
    </p>
    <figure>
      <img src="feature_parameter_size.png" alt="" width="600" height="350" />
      <figcaption>Feature and parameter size (credit Andrew NG).</figcaption>
    </figure>
    <p>
      The contour plot has the horizontal axis has a much narrower range, say between zero and one, whereas the vertical
      axis takes on much larger values, say between 10 and 100. So the contours form ovals or ellipses. This is because
      a very small change to w1 can have a very large impact on the estimated price and that's a very large impact on
      the cost J.
    </p>

    <p>
      Because the contours are so tall and skinny, gradient descent may end up bouncing back and forth for a long time
      before it can finally find its way to the global minimum. In situations like this, a useful thing to do is to
      scale the features.This means performing some transformation of your training data so that x <sub>1</sub> say
      might now range from 0 to 1 and x<sub>2</sub> might also range from 0 to 1.
    </p>
    <figure>
      <img src="scatterplot_rescaled.png" alt="" width="400" height="400" />
      <figcaption>Scatterplot rescaled (credit Andrew NG).</figcaption>
    </figure>
    <p>
      When you have different features that take on very different ranges of values, it can cause gradient descent to
      run slowly but re scaling the different features so they all take on comparable range of values. After rescaling,
      gradient descent can find a much more direct path to the global minimum.
    </p>
    <p>
      The key point is that the re-scaled x <sub>1</sub> and x <sub>2</sub> are both now taking comparable ranges of
      values to each other. If you run gradient descent on a cost function, contours will look more like this more like
      circles and less tall and skinny.
    </p>

    <figure>
      <img src="contour_after_scaling.png" alt="" width="600" height="300" />
      <figcaption>After scaled (credit Andrew NG).</figcaption>
    </figure>

    <p>One of the option to scale is dividing by the maximum</p>
    <figure>
      <img src="scaling_dividing_my_max.png" alt="" width="550" height="310" />
      <figcaption>Dividing by maximum (credit Andrew NG).</figcaption>
    </figure>
    <p>The other option to scale is mean normalization</p>
    <figure>
      <img src="scaling_mean_normalisation.png" alt="" width="550" height="310" />
      <figcaption>Mean normalisation (credit Andrew NG).</figcaption>
    </figure>
    <p>Yet other option to scale is z-score normalization</p>
    <figure>
      <img src="z_score_normalisation.png" alt="" width="550" height="280" />
      <figcaption>Z-Score normalisation (credit Andrew NG).</figcaption>
    </figure>
    <p>There's almost never any harm to carrying out feature re-scaling. When in doubt, you to just carry it out</p>

    <p>
      Following is an example of cases where feature scaling can be applied.In this example, x<sub>5</sub> representing
      temperature values are around 100, which is actually pretty large compared to other scale features, and this will
      actually cause gradient descent to run more slowly. In this case, feature re-scaling will help.
    </p>
    <figure>
      <img src="feature_scaling_numbers.png" alt="" width="420" height="280" />
      <figcaption>Scaling (credit Andrew NG).</figcaption>
    </figure>
    <h2>Checking gradient descent for convergence</h2>

    <p>
      The job of gradient descent is to find parameters w and b that hopefully minimize the cost function J. But when
      running gradient descent, how can you tell if it is converging?
    </p>
    <figure>
      <img src="gradient_discent_objective.png" alt="" width="200" height="70" />
      <figcaption>Gradient discent objective (credit Andrew NG).</figcaption>
    </figure>

    <p>Plotting the value of J at each iteration of gradient descent (Learning curve):</p>

    <figure>
      <img src="gd_iteration_plot.png" alt="" width="500" height="300" />
      <figcaption>Plotting J every iteration (credit Andrew NG).</figcaption>
    </figure>
    <p>
      After you've run gradient descent for 100 iterations, (100 simultaneous updates of the parameters), look into the
      curve to see how your cost J changes after each iteration. If gradient descent is working properly, then the cost
      J should decrease after every single iteration. If J ever increases after one iteration, that means either Alpha
      is chosen poorly, and it usually means Alpha is too large, or there could be a bug in the code.
    </p>

    <p>
      Another obersation is that by the time you reach 300 iterations, the cost J is leveling off and is no longer
      decreasing much. By 400 iterations, it looks like the curve has flattened out. Looking at this learning curve, you
      can try to spot whether or not gradient descent is converging.
    </p>
    <figure>
      <img src="gd_converging.png" alt="" width="700" height="350" />
      <figcaption>Gradient discent converging (credit Andrew NG).</figcaption>
    </figure>
    <p>
      We can actually look at graphs like this rather than rely on automatic convergence tests. If learning rate is too
      small, it will run very slowly and if it is too large, it may not even converge.
    </p>

    <p>
      If you plot the cost for a number of iterations and notice that the costs sometimes goes up and sometimes goes
      down, you should take that as a clear sign that gradient descent is not working properly.
    </p>

    <figure>
      <img src="j_iteration_graph.png" alt="" width="650" height="350" />
      <figcaption>J vs iterations (credit Andrew NG).</figcaption>
    </figure>

    <h2>Common notations</h2>
    <figure>
      <img src="ml_notations.png" alt="" width="1000" height="600" />
      <figcaption>J vs iterations (credit Andrew NG).</figcaption>
    </figure>

    <h2>Feature Engineering</h2>
    <p>
      The choice of features can have a huge impact on your learning algorithm's performance. Choosing or entering the
      right features is a critical step to making the algorithm work well
    </p>

    <p>
      For example plotting each feature vs. the target, price, provides some indication of which features have the
      strongest influence on price. Bedrooms and floors don't seem to have a strong impact on price. Newer houses have
      higher prices than older houses:
    </p>
    <figure>
      <img src="feature_influence.png" alt="" width="500" height="180" />
      <figcaption>Feature influence (credit Andrew NG).</figcaption>
    </figure>

    <p>
      Feature engineering - using intuition to design new features by transforming or combining original features.
      Depending on what insights you may have into the application, you may define new features, and get a much better
      model
    </p>

    <figure>
      <img src="frontage_depth.png" alt="" width="460" height="300" />
      <figcaption>Feature influence (credit Andrew NG).</figcaption>
    </figure>

    <p>
      There is a flavor of feature engineering, that allow you to fit not just straight lines, but curves, non-linear
      functions to your data. Polynomial regression will let you fit curves, non-linear functions, to your data.
    </p>

    <hr />
    <p>
      <a href="../06_ml/06.html" style="color: firebrick"> &#8617; Previous</a>
    </p>
  </body>
</html>