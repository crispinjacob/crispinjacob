<!doctype html>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" type="text/css" href="../../styles/commonstyles.css" />
<link rel="stylesheet" href="../../styles/blogstyle.css" />
<link rel="stylesheet" href="../../styles/iconbarstyle.css" />
<link rel="stylesheet" href="../../styles/toc.css" />
<link rel="stylesheet" href="../../styles/figcaptionstyles.css" />

<html lang="en-us">
  <head>
    <title>Databases</title>
  </head>

  <body>
    <div class="icon-bar">
      <a class="active" href="../../index.html"><i class="fa fa-home"></i></a>
    </div>

    <div style="padding: 20px">
      <p>02 May 2025</p>
      <p>
        <a href="../06_ml/06.html" style="color: firebrick"> &#8617; Previous</a>
      </p>

      <hr />

      <div id="toc"></div>
      <script src="../../scripts/toc.js"></script>
    </div>
    <h2>Feature scaling</h2>
    <p>Consider below model for housing price</p>
    <ul>
      <li>price = w<sub>(1)</sub> x<sub>(1)</sub> + w<sub>(2)</sub> x<sub>(2)</sub> + b</li>
      <li>x<sub>(1)</sub> - size in sq feet (range: 300-2000). Range is large.</li>
      <li>x<sub>(2)</sub> - bedrooms (range:0-5). Range is small.</li>
    </ul>
    <p>
      Applying the largest feature range (2000 and 5) into the model, with model parameters w<sub>(1)</sub> = 50 ,
      w<sub>(2)</sub>= 0.1 , b = 50 :
    </p>

    <ul>
      <li>price = w<sub>(1)</sub> x <sub>(1)</sub> + w<sub>(2)</sub> x <sub>(2)</sub> + b</li>
      <li>
        price = 50 * 2000 + 0.1 * 5 + 50 = $100,050.5. This is far from actual price of 500k dollars. So this is not a
        good parameter choice
      </li>
    </ul>

    <p>Another example with w<sub>(1)</sub> = 0.1 , w<sub>(2)</sub>= 50 , b = 50 :</p>

    <ul>
      <li>price = w<sub>(1)</sub> x <sub>(1)</sub> + w<sub>(2)</sub> x <sub>(2)</sub> + b</li>
      <li>price = 0.1 * 2000 + 0.1 * 5 + 50 = $500,000. This is more reasonable</li>
    </ul>

    <p>
      When the possible range of values of a feature is large a good model will learn to choose a relatively small
      parameter value, like 0.1. Likewise, when the possible values of the feature are small then a reasonable parameter
      value will be relatively large like 50.
    </p>
    <figure>
      <img src="feature_parameter_size.png" alt="" width="600" height="350" />
      <figcaption>Feature and parameter size (credit Andrew NG).</figcaption>
    </figure>
    <p>
      The contour plot has the horizontal axis has a much narrower range, say between zero and one, whereas the vertical
      axis takes on much larger values, say between 10 and 100. So the contours form ovals or ellipses. This is because
      a very small change to w1 can have a very large impact on the estimated price and that's a very large impact on
      the cost J.
    </p>

    <p>
      Because the contours are so tall and skinny, gradient descent may end up bouncing back and forth for a long time
      before it can finally find its way to the global minimum. In situations like this, a useful thing to do is to
      scale the features.This means performing some transformation of your training data so that x <sub>1</sub> say
      might now range from 0 to 1 and x<sub>2</sub> might also range from 0 to 1.
    </p>
    <figure>
      <img src="scatterplot_rescaled.png" alt="" width="400" height="400" />
      <figcaption>Scatterplot rescaled (credit Andrew NG).</figcaption>
    </figure>
    <p>
      When you have different features that take on very different ranges of values, it can cause gradient descent to
      run slowly but re scaling the different features so they all take on comparable range of values. After rescaling,
      gradient descent can find a much more direct path to the global minimum.
    </p>
    <p>
      The key point is that the re-scaled x <sub>1</sub> and x <sub>2</sub> are both now taking comparable ranges of
      values to each other. If you run gradient descent on a cost function, contours will look more like this more like
      circles and less tall and skinny.
    </p>

    <figure>
      <img src="contour_after_scaling.png" alt="" width="600" height="300" />
      <figcaption>After scaled (credit Andrew NG).</figcaption>
    </figure>

    <p>One of the option to scale is dividing by the maximum</p>
    <figure>
      <img src="scaling_dividing_my_max.png" alt="" width="550" height="310" />
      <figcaption>Dividing by maximum (credit Andrew NG).</figcaption>
    </figure>
    <p>The other option to scale is mean normalization</p>
    <figure>
      <img src="scaling_mean_normalisation.png" alt="" width="550" height="310" />
      <figcaption>Mean normalisation (credit Andrew NG).</figcaption>
    </figure>
    <p>Yet other option to scale is z-score normalization</p>
    <figure>
      <img src="z_score_normalisation.png" alt="" width="550" height="280" />
      <figcaption>Z-Score normalisation (credit Andrew NG).</figcaption>
    </figure>
    <p>There's almost never any harm to carrying out feature re-scaling. When in doubt, you to just carry it out</p>

    <p>
      Following is an example of cases where feature scaling can be applied.In this example, x<sub>5</sub> representing
      temperature values are around 100, which is actually pretty large compared to other scale features, and this will
      actually cause gradient descent to run more slowly. In this case, feature re-scaling will help.
    </p>
    <figure>
      <img src="feature_scaling_numbers.png" alt="" width="420" height="280" />
      <figcaption>Scaling (credit Andrew NG).</figcaption>
    </figure>
    <h2>Checking gradient descent for convergence</h2>

    <p>
      The job of gradient descent is to find parameters w and b that hopefully minimize the cost function J. But when
      running gradient descent, how can you tell if it is converging?
    </p>
    <figure>
      <img src="gradient_discent_objective.png" alt="" width="200" height="70" />
      <figcaption>Gradient discent objective (credit Andrew NG).</figcaption>
    </figure>

    <p>Plotting the value of J at each iteration of gradient descent (Learning curve):</p>

    <figure>
      <img src="gd_iteration_plot.png" alt="" width="500" height="300" />
      <figcaption>Plotting J every iteration (credit Andrew NG).</figcaption>
    </figure>
    <p>
      After you've run gradient descent for 100 iterations, (100 simultaneous updates of the parameters), look into the
      curve to see how your cost J changes after each iteration. If gradient descent is working properly, then the cost
      J should decrease after every single iteration. If J ever increases after one iteration, that means either Alpha
      is chosen poorly, and it usually means Alpha is too large, or there could be a bug in the code.
    </p>

    <p>
      Another obersation is that by the time you reach 300 iterations, the cost J is leveling off and is no longer
      decreasing much. By 400 iterations, it looks like the curve has flattened out. Looking at this learning curve, you
      can try to spot whether or not gradient descent is converging.
    </p>
    <figure>
      <img src="gd_converging.png" alt="" width="700" height="350" />
      <figcaption>Gradient discent converging (credit Andrew NG).</figcaption>
    </figure>
    <p>
      We can actually look at graphs like this rather than rely on automatic convergence tests. If learning rate is too
      small, it will run very slowly and if it is too large, it may not even converge.
    </p>

    <p>
      If you plot the cost for a number of iterations and notice that the costs sometimes goes up and sometimes goes
      down, you should take that as a clear sign that gradient descent is not working properly.
    </p>

    <figure>
      <img src="j_iteration_graph.png" alt="" width="650" height="350" />
      <figcaption>J vs iterations (credit Andrew NG).</figcaption>
    </figure>

    <h2>Common notations</h2>
    <figure>
      <img src="ml_notations.png" alt="" width="1000" height="600" />
      <figcaption>J vs iterations (credit Andrew NG).</figcaption>
    </figure>

    <h2>Feature Engineering</h2>
    <p>
      The choice of features can have a huge impact on your learning algorithm's performance. Choosing or entering the
      right features is a critical step to making the algorithm work well
    </p>

    <p>
      For example plotting each feature vs. the target, price, provides some indication of which features have the
      strongest influence on price. Bedrooms and floors don't seem to have a strong impact on price. Newer houses have
      higher prices than older houses:
    </p>
    <figure>
      <img src="feature_influence.png" alt="" width="500" height="180" />
      <figcaption>Feature influence (credit Andrew NG).</figcaption>
    </figure>

    <p>
      Feature engineering - using intuition to design new features by transforming or combining original features.
      Depending on what insights you may have into the application, you may define new features, and get a much better
      model
    </p>

    <figure>
      <img src="frontage_depth.png" alt="" width="460" height="300" />
      <figcaption>Feature influence (credit Andrew NG).</figcaption>
    </figure>
    <h2>Polynomial regression</h2>
    <p>
      There is a flavor of feature engineering, that allow you to fit not just straight lines, but curves, non-linear
      functions to your data. Polynomial regression will let you fit curves, non-linear functions, to your data.
    </p>

    <p>
      A polynomial is a general algebraic expression involving variables and coefficients, typically containing multiple
      terms and different powers of the variable. A quadratic is a specific type of polynomial where the highest power
      of the variable is 2, meaning it's a second-degree polynomial
    </p>

    <ul>
      <li>f(x)=a<sub>n</sub>x<sup>n</sup>+a<sub>n-1</sub>x<sup>n-1</sup>+⋯+a<sub>1</sub>x+a0 - polinomial</li>
      <li>f(x)=ax<sup>2</sup>+bx+c quadratic</li>
    </ul>
    <p>Sometimes quadratic model doesn't really make sense because a quadratic function eventually comes back down</p>

    <figure>
      <img src="quadratic.png" alt="" width="460" height="300" />
      <figcaption>Quadratic (credit Andrew NG).</figcaption>
    </figure>
    <p>
      A cubic function may be a better fit to the data because the size does eventually come back up as the size
      increases. In the case of the cubic function, the first feature is the size, the second feature is the size
      squared, and the third feature is the size cubed.
    </p>

    <figure>
      <img src="cubic.png" alt="" width="700" height="300" />
      <figcaption>Cubic (credit Andrew NG).</figcaption>
    </figure>
    <p>
      If you create features that are these powers like the square of the original features like this, then feature
      scaling becomes increasingly important. If the size of the house ranges from say, 1-1,000 square feet, then the
      second feature, which is a size squared, will range from one to a million, and the third feature, which is size
      cubed, ranges from one to a billion.
    </p>
    <p>Another reasonable alternative to taking the size squared and size cubed is to say use the square root of x</p>

    <figure>
      <img src="square_root.png" alt="" width="700" height="300" />
      <figcaption>Cubic (credit Andrew NG).</figcaption>
    </figure>

    <h2>Logistic regression</h2>
    <p>
      In classification, output variable y can take on only one of a small handful of possible values instead of any
      number in an infinite range of numbers. Linear regression can be used to predict a number. Linear regression is
      not a good algorithm for classification problems. This will lead us into a different algorithm called logistic
      regression which is one of the most popular and most widely used learning algorithms today
    </p>
    <p>
      The type of classification problem where there are only two possible outputs is called binary classification.
      Example "Is this email a spam?", "Is the tumor is malignant?"
    </p>

    <p>
      Linear regression predicts not just the values zero and one, but all numbers between zero and one or even less
      than zero or greater than one. But here we want to predict categories
    </p>

    <figure>
      <img src="linear_regression_not_suitable_1.png" alt="" width="600" height="350" />
      <figcaption>Cubic (credit Andrew NG).</figcaption>
    </figure>

    <p>
      If you draw vertical line , everything to the left ends up with a prediction of y equals zero. And everything on
      the right ends up with the prediction of y equals one.
    </p>

    <p>
      What happens if your dataset has one more training example (right side marked as x) ? Adding that example
      shouldn't change any of our conclusions about how to classify malignant versus benign tumors. When tumor is large,
      we want the algorithm to classify it as malignant
    </p>
    <figure>
      <img src="linear_regression_not_suitable_2.png" alt="" width="700" height="400" />
      <figcaption>Cubic (credit Andrew NG).</figcaption>
    </figure>
    <p>
      Logistic regression helps in this situation. Even though it has the word of regression in it is actually used for
      classification. The name was given for historical reasons. It is used to solve binary classification problems with
      output label y is either zero or one. In logistic regression we fit an S-shaped curve.
    </p>

    <figure>
      <img src="s_shaped_curve.png" alt="" width="400" height="300" />
      <figcaption>Sigmoid (credit Andrew NG).</figcaption>
    </figure>
    <p>
      To build out to the logistic regression algorithm, there's an important mathematical function called the Sigmoid
      function, sometimes also referred to as the logistic function. It helps in outputing between 0 and 1.
    </p>
    <figure>
      <img src="sigmoid_between_0_and_1.png" alt="" width="350" height="350" />
      <figcaption>Sigmoid (credit Andrew NG).</figcaption>
    </figure>

    <p>
      Horizontal axis takes on both negative and positive values here. "e" is a mathematical constant that takes on a
      value of about 2.7. When z is small (say -100) , g(z) will become closer to 0. When z is large (say 100), g(z) is
      going to be very close to 1.
    </p>
    <p>In linear regression, f(x) = w*x+b</p>
    <p>
      In logistic regression, variable z refers to the linear combination of the input features and model parameters.
      It's the input to the sigmoid function Mathematicallyt it is
    </p>
    <p>z = w*x+b</p>
    <p>
      For example, x= hours predict if a student will pass based on study hours. x= hours studied, and predict whether
      the student passes (1) or fails (0). Let’s assume our model has learned: Weight: w=2, Bias: b=−4. Then
    </p>

    <p>z= w*x+b = 2*3+(-4) = 2</p>
    <p>
      Applying Sigmoid function g(z) = g(2) = 0.88. The model predicts a probability of 88% that the student will pass.
      If your threshold is 0.5, you'd classify this student as likely to pass.
    </p>

    <figure>
      <img src="sigmoid_probabiblity.png" alt="" width="350" height="250" />
      <figcaption>Probability (credit Andrew NG).</figcaption>
    </figure>
    <p>If the chance of y being 1 is 0.7 (70 percent), then the chance of it being 0 has got to be 0.3 (30 percent)</p>

    <p>
      For a long time, a lot of Internet advertising was actually driven by a slight variation of logistic regression,
      that decided what ad was shown to you and many others on some large websites.
    </p>

    <h3>Decision boundary</h3>
    <p>
      Here is a training set where the red crosses denote the positive examples and the blue circles denote negative
      examples The red crosses corresponds to y equals 1, and the blue circles correspond to y equals 0. Here decision
      boundary is a straight line.
    </p>

    <figure>
      <img src="decision_boundary_straingt_line.png" alt="" width="350" height="250" />
      <figcaption>Decision boundary as straght line (credit Andrew NG).</figcaption>
    </figure>

    <p>More complex examples may not have decision boundary as a straight line. It can be non-linear.</p>
    <figure>
      <img src="nonlinear_decision_boundary_1.png" alt="" width="600" height="250" />
      <figcaption>Complex decision boundary (credit Andrew NG).</figcaption>
    </figure>

    <p>
      Even more complex decision boundaries can be made by even higher-order polynomial terms. Logistic regression can
      learn to fit pretty complex data.
    </p>
    <figure>
      <img src="nonlinear_decision_boundary_2.png" alt="" width="600" height="300" />
      <figcaption>More complex decision boundary (credit Andrew NG).</figcaption>
    </figure>

    <h3>Cost function for logistic regression</h3>
    <p>
      Squared error cost function is not an ideal cost function for logistic regression. In linear regression, cost
      function looks like a convex function or a bowl shape or hammer shape. Now you could try to use the same cost
      function for logistic regression.
    </p>

    <figure>
      <img src="logistic_cost_function.png" alt="" width="700" height="250" />
      <figcaption>Logistic regression cost function (credit Andrew NG).</figcaption>
    </figure>

    <figure>
      <img src="logistic_cost_function.png" alt="" width="700" height="250" />
      <figcaption>Logistic regression cost function (credit Andrew NG).</figcaption>
    </figure>
    <p>
      This becomes what's called a non-convex cost function. if you were to try to use gradient descent, there are lots
      of local minima that you can get into trouble. It turns out that for logistic regression, this squared error cost
      function is not a good choice. Instead, there will be a different cost function that can make the cost function
      convex again.
    </p>
    <p>
      A loss function measures how well you're doing on one training example. By summing up the losses on all of the
      training examples you then get, the cost function, which measures how well you're doing on the entire training
      set.
    </p>
    <figure>
      <img src="logistic_loss_function_2.png" alt="" width="700" height="300" />
      <figcaption>Logistic loss function (credit Andrew NG).</figcaption>
    </figure>

    <p>
      It turns out that with this choice of loss function, the overall cost function will be convex and thus you can
      reliably use gradient descent to take you to the global minimum
    </p>
    <p>
      Cost function is a function of the entire training set and is, therefore, the average or 1 over m times the sum of
      the loss function on the individual training examples
    </p>
    <figure>
      <img src="logistic_cost_function_2.png" alt="" width="600" height="250" />
      <figcaption>Logistic loss function (credit Andrew NG).</figcaption>
    </figure>

    <h3>Simplified cost function for logistic regression</h3>
    <figure>
      <img src="logistics_simplified_cost.png" alt="" width="700" height="250" />
      <figcaption>Simplified cost function (credit Andrew NG).</figcaption>
    </figure>

    <h3>Gradient descent with logistic regression</h3>

    <p>
      To fit the parameters of a logistic regression model , we have to find the values of the parameters w and b that
      minimize the cost function J.
    </p>

    <figure>
      <img src="gd_logistics.png" alt="" width="700" height="300" />
      <figcaption>Gradient descent for logistic regression (credit Andrew NG).</figcaption>
    </figure>
    <figure>
      <img src="gd_logistics_2.png" alt="" width="700" height="320" />
      <figcaption>Gradient descent for logistic regression (credit Andrew NG).</figcaption>
    </figure>

    <p>
      In feature scaling for linear regression, we scaled all the features to take on similar ranges of values, say
      between negative 1 and plus 1. Feature scaling applied the same way to logistic regression to speed up gradient
      descent.
    </p>
    <h2>Overfitting and underfitting</h2>

    <p>
      Underfitting means that the mode is just not even able to fit the training set that well. Another term for
      underfitting is the "algorithm has high bias".
    </p>

    <p>
      A good fit is called generalize. Technically we say that you want your learning algorithm to generalize well,
      which means to make good predictions
    </p>

    <p>
      Sometimes, the algorithm can run into a problem called overfitting, which can cause it to perform poorly. The
      model does extremely good job fitting the training data. It passes through all of the training data perfectly. The
      cost function being exactly equal to zero because the errors are zero.But this is a very wiggly curve, its going
      up and down all over the place. Another term for this is that the algorithm has high variance.In machine learning,
      many people will use the terms over-fit and high-variance almost interchangeably.
    </p>
    <p>
      There are some techniques like regularization to help in minimizing overfitting problem and get the learning
      algorithms to work much better.
    </p>
    <figure>
      <img src="underfit_overfit.png" alt="" width="700" height="320" />
      <figcaption>Overfitting and underfitting (credit Andrew NG).</figcaption>
    </figure>

    <h3>Overfitting in classification</h3>
    <p>Overfitting applies a classification as well. For example classify if a tumor is malignant or benign.</p>

    <figure>
      <img src="overfitting_in_classification.png" alt="" width="700" height="320" />
      <figcaption>Overfitting in classification (credit Andrew NG).</figcaption>
    </figure>

    <h3>Addressing overfitting</h3>

    <p>1. Collect more training examples</p>
    <p>
      One way to address this problem is to collect more training data.With the larger training set, the learning
      algorithm will learn to fit a function that is less wiggly. You can continue to fit a high order polynomial or
      some of the function with a lot of features, and if you have enough training examples, it will still do okay.
    </p>
    <figure>
      <img src="collect_training_examples.png" alt="" width="700" height="320" />
      <figcaption>Collect more training examples (credit Andrew NG).</figcaption>
    </figure>
    <p>2. use fewer features</p>
    <p>
      Getting more data isn't always an option. A second option for addressing overfitting is to see if you can use
      fewer features. If you have a lot of features but don't have enough training data, then your learning algorithm
      may also overfit to your training set.Choosing the most appropriate set of features to use is sometimes also
      called feature selection. One disadvantage of feature selection is that by using only a subset of the features,
      the algorithm is throwing away some of the information. Regularization is a way to more gently reduce the impacts
      of some of the features without doing something as harsh as eliminating it outright
    </p>

    <figure>
      <img src="reduce_features.png" alt="" width="700" height="320" />
      <figcaption>Reducing features (credit Andrew NG).</figcaption>
    </figure>
    <p>3. Regularisation</p>
    <p>
      Regularisation lets you keep all of your features, but they just prevents the features from having an overly large
      effect, which is what sometimes can cause overfitting. It reduces the size of the parameters. Regularization is a
      very frequently used technique.
    </p>

    <figure>
      <img src="regularisation.png" alt="" width="600" height="320" />
      <figcaption>Regularisation (credit Andrew NG).</figcaption>
    </figure>
    <h2>Cost function with regularization</h2>
    <p>
      If you fit a quadratic function to a sample data, it gives a pretty good fit. But if you fit a very high order
      polynomial, you end up with a curve that over fits the data
    </p>
    <figure>
      <img src="cost_function_with_regularisation.png" alt="" width="600" height="320" />
      <figcaption>Regularisation (credit Andrew NG).</figcaption>
    </figure>
    <p>
      In the above example 1000 is chosen as a larger number.So with this modified cost function, you are penalizing the
      model if w3 and w4 are large. Because if you want to minimize this function, the only way to make this new cost
      function small is if w3 and w4 are both small. So when you minimize this function, you are going to end up with w3
      close to 0 and w4 close to 0. So we're effectively nearly canceling out the effects of the features x
      <sup>3</sup> and x <sup>4</sup> and getting rid of these two terms. The idea is that if there are smaller values
      for the parameters, then that's a bit like having a simpler model.
    </p>

    <p>
      If you have a lot of features, say a 100 features, you may not know which are the most important features and
      which ones to penalize. So the way regularization is typically implemented is to penalize all of the features.
      Let's penalize all of them a bit and shrink all of them by adding this new term lambda.We also divide lambda by 2m
      so that both the 1st and 2nd terms here are scaled by 1/2m.
    </p>

    <figure>
      <img src="regularisation_lambda.png" alt="" width="800" height="400" />
      <figcaption>Regularisation (credit Andrew NG).</figcaption>
    </figure>
    <p>
      If lambda is very, very large, the learning algorithm will choose w1, w2, w3 and w4 to be extremely close to 0 and
      thus f(x) is basically equal to b and so the learning algorithm fits a horizontal straight line and under fits
    </p>

    <figure>
      <img src="regularisation_underfit.png" alt="" width="600" height="350" />
      <figcaption>Regularisation (credit Andrew NG).</figcaption>
    </figure>
    <h2>Regularized linear regression</h2>
    <figure>
      <img src="regularised_linear_regression.png" alt="" width="600" height="350" />
      <figcaption>Regularized linear regression (credit Andrew NG).</figcaption>
    </figure>

    <p>
      What regularization is doing on every single iteration is you're multiplying w by a number slightly less than 1
      and that has effect of shrinking the value of w<sub>j</sub> just a little bit
    </p>
    <h2>Regularized logistic regression</h2>
    <p>
      Gradient update for logistic regression is similar to the gradient update for linear regression. Similarly the gradient descent update for regularized logistic regression will also look similar to the update for regularized linear regression
    </p>
    <figure>
      <img src="reguarised_logistic_regression.png" alt="" width="600" height="300" />
      <figcaption>Regularized linear regression (credit Andrew NG).</figcaption>
    </figure>
    
    
    <hr />
    <p>
      <a href="../06_ml/06.html" style="color: firebrick"> &#8617; Previous</a>
    </p>
    <p></p>
  </body>
</html>