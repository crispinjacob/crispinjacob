<!doctype html>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" type="text/css" href="../../styles/commonstyles.css" />
<link rel="stylesheet" href="../../styles/blogstyle.css" />
<link rel="stylesheet" href="../../styles/iconbarstyle.css" />
<link rel="stylesheet" href="../../styles/toc.css" />
<link rel="stylesheet" href="../../styles/figcaptionstyles.css" />

<html lang="en-us">
  <head>
    <title>Databases</title>
  </head>

  <body>
    <div class="icon-bar">
      <a class="active" href="../../index.html"><i class="fa fa-home"></i></a>
    </div>

    <div style="padding: 20px">
      <p>02 May 2025</p>
      <p>
        <a href="../06_ml/06.html" style="color: firebrick"> &#8617; Previous</a>
      </p>

      <hr />

      <div id="toc"></div>
      <script src="../../scripts/toc.js"></script>
    </div>
    <h2>Feature scaling</h2>
    <p>Consider below model for housing price</p>
    <ul>
      <li>price = w<sub>(1)</sub> x<sub>(1)</sub> + w<sub>(2)</sub> x<sub>(2)</sub> + b</li>
      <li>x<sub>(1)</sub> - size in sq feet (range: 300-2000). Range is large.</li>
      <li>x<sub>(2)</sub> - bedrooms (range:0-5). Range is small.</li>
    </ul>
    <p>
      Applying the largest feature range (2000 and 5) into the model, with model parameters w<sub>(1)</sub> = 50 ,
      w<sub>(2)</sub>= 0.1 , b = 50 :
    </p>

    <ul>
      <li>price = w<sub>(1)</sub> x <sub>(1)</sub> + w<sub>(2)</sub> x <sub>(2)</sub> + b</li>
      <li>
        price = 50 * 2000 + 0.1 * 5 + 50 = $100,050.5. This is far from actual price of 500k dollars. So this is not a
        good parameter choice
      </li>
    </ul>

    <p>Another example with w<sub>(1)</sub> = 0.1 , w<sub>(2)</sub>= 50 , b = 50 :</p>

    <ul>
      <li>price = w<sub>(1)</sub> x <sub>(1)</sub> + w<sub>(2)</sub> x <sub>(2)</sub> + b</li>
      <li>price = 0.1 * 2000 + 0.1 * 5 + 50 = $500,000. This is more reasonable</li>
    </ul>

    <p>
      When the possible range of values of a feature is large a good model will learn to choose a relatively small
      parameter value, like 0.1. Likewise, when the possible values of the feature are small then a reasonable parameter
      value will be relatively large like 50.
    </p>
    <figure>
      <img src="feature_parameter_size.png" alt="" width="600" height="350" />
      <figcaption>Feature and parameter size (credit Andrew NG).</figcaption>
    </figure>
    <p>
      The contour plot has the horizontal axis has a much narrower range, say between zero and one, whereas the vertical
      axis takes on much larger values, say between 10 and 100. So the contours form ovals or ellipses. This is because
      a very small change to w1 can have a very large impact on the estimated price and that's a very large impact on
      the cost J.
    </p>

    <p>
      Because the contours are so tall and skinny, gradient descent may end up bouncing back and forth for a long time
      before it can finally find its way to the global minimum. In situations like this, a useful thing to do is to
      scale the features.This means performing some transformation of your training data so that x <sub>1</sub> say
      might now range from 0 to 1 and x<sub>2</sub> might also range from 0 to 1.
    </p>
    <figure>
      <img src="scatterplot_rescaled.png" alt="" width="400" height="400" />
      <figcaption>Scatterplot rescaled (credit Andrew NG).</figcaption>
    </figure>
    <p>
      When you have different features that take on very different ranges of values, it can cause gradient descent to
      run slowly but re scaling the different features so they all take on comparable range of values. After rescaling,
      gradient descent can find a much more direct path to the global minimum.
    </p>
    <p>
      The key point is that the re-scaled x <sub>1</sub> and x <sub>2</sub> are both now taking comparable ranges of
      values to each other. If you run gradient descent on a cost function, contours will look more like this more like
      circles and less tall and skinny.
    </p>

    <figure>
      <img src="contour_after_scaling.png" alt="" width="600" height="300" />
      <figcaption>After scaled (credit Andrew NG).</figcaption>
    </figure>

    <p>One of the option to scale is dividing by the maximum</p>
    <figure>
      <img src="scaling_dividing_my_max.png" alt="" width="550" height="310" />
      <figcaption>Dividing by maximum (credit Andrew NG).</figcaption>
    </figure>
    <p>The other option to scale is mean normalization</p>
    <figure>
      <img src="scaling_mean_normalisation.png" alt="" width="550" height="310" />
      <figcaption>Mean normalisation (credit Andrew NG).</figcaption>
    </figure>
    <p>Yet other option to scale is z-score normalization</p>
    <figure>
      <img src="z_score_normalisation.png" alt="" width="550" height="280" />
      <figcaption>Z-Score normalisation (credit Andrew NG).</figcaption>
    </figure>
    <p>There's almost never any harm to carrying out feature re-scaling. When in doubt, you to just carry it out</p>

    <p>
      Following is an example of cases where feature scaling can be applied.In this example, x<sub>5</sub> representing
      temperature values are around 100, which is actually pretty large compared to other scale features, and this will
      actually cause gradient descent to run more slowly. In this case, feature re-scaling will help.
    </p>
    <figure>
      <img src="feature_scaling_numbers.png" alt="" width="420" height="280" />
      <figcaption>Scaling (credit Andrew NG).</figcaption>
    </figure>
    <h2>Checking gradient descent for convergence</h2>

    <p>
      The job of gradient descent is to find parameters w and b that hopefully minimize the cost function J. But when
      running gradient descent, how can you tell if it is converging?
    </p>
    <figure>
      <img src="gradient_discent_objective.png" alt="" width="200" height="70" />
      <figcaption>Gradient discent objective (credit Andrew NG).</figcaption>
    </figure>

    <p>Plotting the value of J at each iteration of gradient descent (Learning curve):</p>

    <figure>
      <img src="gd_iteration_plot.png" alt="" width="500" height="300" />
      <figcaption>Plotting J every iteration (credit Andrew NG).</figcaption>
    </figure>
    <p>
      After you've run gradient descent for 100 iterations, (100 simultaneous updates of the parameters), look into the
      curve to see how your cost J changes after each iteration. If gradient descent is working properly, then the cost
      J should decrease after every single iteration. If J ever increases after one iteration, that means either Alpha
      is chosen poorly, and it usually means Alpha is too large, or there could be a bug in the code.
    </p>

    <p>
      Another obersation is that by the time you reach 300 iterations, the cost J is leveling off and is no longer
      decreasing much. By 400 iterations, it looks like the curve has flattened out. Looking at this learning curve, you
      can try to spot whether or not gradient descent is converging.
    </p>
    <figure>
      <img src="gd_converging.png" alt="" width="700" height="350" />
      <figcaption>Gradient discent converging (credit Andrew NG).</figcaption>
    </figure>
    <p>
      We can actually look at graphs like this rather than rely on automatic convergence tests. If learning rate is too
      small, it will run very slowly and if it is too large, it may not even converge.
    </p>

    <p>
      If you plot the cost for a number of iterations and notice that the costs sometimes goes up and sometimes goes
      down, you should take that as a clear sign that gradient descent is not working properly.
    </p>

    <figure>
      <img src="j_iteration_graph.png" alt="" width="650" height="350" />
      <figcaption>J vs iterations (credit Andrew NG).</figcaption>
    </figure>

    <h2>Common notations</h2>
    <figure>
      <img src="ml_notations.png" alt="" width="1000" height="600" />
      <figcaption>J vs iterations (credit Andrew NG).</figcaption>
    </figure>

    <h2>Feature Engineering</h2>
    <p>
      The choice of features can have a huge impact on your learning algorithm's performance. Choosing or entering the
      right features is a critical step to making the algorithm work well
    </p>

    <p>
      For example plotting each feature vs. the target, price, provides some indication of which features have the
      strongest influence on price. Bedrooms and floors don't seem to have a strong impact on price. Newer houses have
      higher prices than older houses:
    </p>
    <figure>
      <img src="feature_influence.png" alt="" width="500" height="180" />
      <figcaption>Feature influence (credit Andrew NG).</figcaption>
    </figure>

    <p>
      Feature engineering - using intuition to design new features by transforming or combining original features.
      Depending on what insights you may have into the application, you may define new features, and get a much better
      model
    </p>

    <figure>
      <img src="frontage_depth.png" alt="" width="460" height="300" />
      <figcaption>Feature influence (credit Andrew NG).</figcaption>
    </figure>
    <h2>Polynomial regression</h2>
    <p>
      There is a flavor of feature engineering, that allow you to fit not just straight lines, but curves, non-linear
      functions to your data. Polynomial regression will let you fit curves, non-linear functions, to your data.
    </p>

    <p>
      A polynomial is a general algebraic expression involving variables and coefficients, typically containing multiple
      terms and different powers of the variable. A quadratic is a specific type of polynomial where the highest power
      of the variable is 2, meaning it's a second-degree polynomial
    </p>

    <ul>
      <li>f(x)=a<sub>n</sub>x<sup>n</sup>+a<sub>n-1</sub>x<sup>n-1</sup>+â‹¯+a<sub>1</sub>x+a0 - polinomial</li>
      <li>f(x)=ax<sup>2</sup>+bx+c quadratic</li>
    </ul>
    <p>Sometimes quadratic model doesn't really make sense because a quadratic function eventually comes back down</p>

    <figure>
      <img src="quadratic.png" alt="" width="460" height="300" />
      <figcaption>Quadratic (credit Andrew NG).</figcaption>
    </figure>
    <p>
      A cubic function may be a better fit to the data because the size does eventually come back up as the size
      increases. In the case of the cubic function, the first feature is the size, the second feature is the size
      squared, and the third feature is the size cubed.
    </p>

    <figure>
      <img src="cubic.png" alt="" width="700" height="300" />
      <figcaption>Cubic (credit Andrew NG).</figcaption>
    </figure>
    <p>
      If you create features that are these powers like the square of the original features like this, then feature
      scaling becomes increasingly important. If the size of the house ranges from say, 1-1,000 square feet, then the
      second feature, which is a size squared, will range from one to a million, and the third feature, which is size
      cubed, ranges from one to a billion.
    </p>
    <p>Another reasonable alternative to taking the size squared and size cubed is to say use the square root of x</p>

    <figure>
      <img src="square_root.png" alt="" width="700" height="300" />
      <figcaption>Cubic (credit Andrew NG).</figcaption>
    </figure>

    <h2>Logistic regression</h2>
    <p>
      In classification, output variable y can take on only one of a small handful of possible values instead of any
      number in an infinite range of numbers. Linear regression can be used to predict a number. Linear regression is
      not a good algorithm for classification problems. This will lead us into a different algorithm called logistic
      regression which is one of the most popular and most widely used learning algorithms today
    </p>
    <p>
      The type of classification problem where there are only two possible outputs is called binary classification.
      Example "Is this email a spam?", "Is the tumor is malignant?"
    </p>

    <p>
      Linear regression predicts not just the values zero and one, but all numbers between zero and one or even less
      than zero or greater than one. But here we want to predict categories
    </p>

    <figure>
      <img src="linear_regression_not_suitable_1.png" alt="" width="600" height="350" />
      <figcaption>Cubic (credit Andrew NG).</figcaption>
    </figure>

    <p>
      If you draw vertical line , everything to the left ends up with a prediction of y equals zero. And everything on
      the right ends up with the prediction of y equals one.
    </p>

    <p>
      What happens if your dataset has one more training example (right side marked as x) ? Adding that example
      shouldn't change any of our conclusions about how to classify malignant versus benign tumors. When tumor is large,
      we want the algorithm to classify it as malignant
    </p>
    <figure>
      <img src="linear_regression_not_suitable_2.png" alt="" width="700" height="400" />
      <figcaption>Cubic (credit Andrew NG).</figcaption>
    </figure>
    <p>
      Logistic regression helps in this situation. Even though it has the word of regression in it is actually used for
      classification. The name was given for historical reasons. It is used to solve binary classification problems with
      output label y is either zero or one. In logistic regression we fit an S-shaped curve.
    </p>

    <figure>
      <img src="s_shaped_curve.png" alt="" width="400" height="300" />
      <figcaption>Cubic (credit Andrew NG).</figcaption>
    </figure>
    <p>
      To build out to the logistic regression algorithm, there's an important mathematical function called the Sigmoid
      function, sometimes also referred to as the logistic function
    </p>

    <hr />
    <p>
      <a href="../06_ml/06.html" style="color: firebrick"> &#8617; Previous</a>
    </p>
    <p></p>
  </body>
</html>